{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "653621a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5db0a8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "#QA\n",
    "inputs = [\n",
    "    \"What is the name of the first chapter in this pdf\",\n",
    "    \"what is the atomic number of hydrogen?\",\n",
    "    \"Total number of chapters in this document?\",\n",
    "]\n",
    "\n",
    "outputs = [\n",
    "    \"The first chapter is about Physical Quantities and Measurement.\",\n",
    "    \"The atomic number of hydrogen is 1.\",\n",
    "    \"There are a total of 16 chapters in this document.\"\n",
    "]\n",
    "\n",
    "# dataset\n",
    "qa_pairs = [{\"question\": q, \"answer\": a} for q, a in zip(inputs, outputs)]\n",
    "df = pd.DataFrame(qa_pairs)\n",
    "\n",
    "# write to csv\n",
    "csv_path = \"../data/goldens.csv\"\n",
    "df.to_csv(csv_path, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3836fd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from langsmith import Client \\n\\nclient = Client()\\ndataset_name = \"llmops_dataset_new0\"\\n\\n# Store \\ndataset = client.create_dataset(\\n    dataset_name=dataset_name,\\n    description=\"input and expected output pairs for llmops series\",\\n)\\n\\nclient.create_examples(\\n    inputs = [{\"question\": q} for q in inputs],\\n    outputs = [{\"answer\": a} for a in outputs],\\n    dataset_id=dataset.id,\\n)'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"from langsmith import Client \n",
    "\n",
    "client = Client()\n",
    "dataset_name = \"llmops_dataset_new0\"\n",
    "\n",
    "# Store \n",
    "dataset = client.create_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    description=\"input and expected output pairs for llmops series\",\n",
    ")\n",
    "\n",
    "client.create_examples(\n",
    "    inputs = [{\"question\": q} for q in inputs],\n",
    "    outputs = [{\"answer\": a} for a in outputs],\n",
    "    dataset_id=dataset.id,\n",
    ")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d613b1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"D:\\\\Machine learning\\\\llmops_series\")\n",
    "\n",
    "from pathlib import Path\n",
    "from src.document_ingestion.data_ingestion import ChatIngestor\n",
    "from src.document_chat.retrieval import ConversationalRAG \n",
    "import os\n",
    "\n",
    "\n",
    "# Simple file adapter for local file paths\n",
    "class LocalFileAdapter:\n",
    "    \"\"\"Adapter for local file paths to work with ChatIngestor.\"\"\"\n",
    "    def __init__(self, file_path: str):\n",
    "        self.path = Path(file_path)\n",
    "        self.name = self.path.name\n",
    "        self.filename = self.path.name  # Add filename attribute for compatibility\n",
    "    \n",
    "    def read(self) -> bytes:\n",
    "        \"\"\"Read method for file-like interface.\"\"\"\n",
    "        return self.path.read_bytes()\n",
    "    \n",
    "    def getbuffer(self) -> bytes:\n",
    "        \"\"\"Get buffer method for compatibility.\"\"\"\n",
    "        return self.path.read_bytes()\n",
    "\n",
    "\n",
    "def answer_ai_report_question(\n",
    "    inputs: dict,\n",
    "    data_path: str = \"D://Machine learning/llmops_series/data/Physics 9th Ch 1 Final.pdf\",\n",
    "    chunk_size: int = 1000,\n",
    "    chunk_overlap: int = 200,\n",
    "    k: int = 5\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Answer questions about the AI Engineering Report using RAG.\n",
    "    \n",
    "    Args:\n",
    "        inputs: Dictionary containing the question, e.g., {\"question\": \"What is RAG?\"}\n",
    "        data_path: Path to the AI Engineering Report text file\n",
    "        chunk_size: Size of text chunks for splitting\n",
    "        chunk_overlap: Overlap between chunks\n",
    "        k: Number of documents to retrieve\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with the answer, e.g., {\"answer\": \"RAG stands for...\"}\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Extract question from inputs\n",
    "        question = inputs.get(\"question\", \"\")\n",
    "        if not question:\n",
    "            return {\"answer\": \"No question provided\"}\n",
    "        \n",
    "        # Check if file exists\n",
    "        if not Path(data_path).exists():\n",
    "            return {\"answer\": f\"Data file not found: {data_path}\"}\n",
    "        \n",
    "        # Create file adapter\n",
    "        file_adapter = LocalFileAdapter(data_path)\n",
    "        \n",
    "        # Build index using ChatIngestor\n",
    "        ingestor = ChatIngestor(\n",
    "            temp_base=\"data\",\n",
    "            faiss_base=\"faiss_index\",\n",
    "            use_session_dirs=True\n",
    "        )\n",
    "        \n",
    "        # Build retriever\n",
    "        ingestor.built_retriever(\n",
    "            uploaded_files=[file_adapter],\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            k=k\n",
    "        )\n",
    "        \n",
    "        # Get session ID and index path\n",
    "        session_id = ingestor.session_id\n",
    "        index_path = f\"faiss_index/{session_id}\"\n",
    "        \n",
    "        # Create RAG instance and load retriever\n",
    "        rag = ConversationalRAG(session_id=session_id)\n",
    "        rag.load_retriever_from_faiss(\n",
    "            index_path=index_path,\n",
    "            k=k,\n",
    "            index_name=os.getenv(\"FAISS_INDEX_NAME\", \"index\")\n",
    "        )\n",
    "        \n",
    "        # Get answer\n",
    "        answer = rag.invoke(question, chat_history=[])\n",
    "        \n",
    "        return {\"answer\": answer}\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\"answer\": f\"Error: {str(e)}\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18b50990",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"timestamp\": \"2026-01-17T05:45:42.063824Z\", \"level\": \"info\", \"event\": \"Running in LOCAL mode: .env loaded\"}\n",
      "{\"timestamp\": \"2026-01-17T05:45:42.064839Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from individual env var\"}\n",
      "{\"timestamp\": \"2026-01-17T05:45:42.067923Z\", \"level\": \"info\", \"event\": \"Loaded GOOGLE_API_KEY from individual env var\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_Kv...\", \"GOOGLE_API_KEY\": \"AIzaSy...\"}, \"timestamp\": \"2026-01-17T05:45:42.069446Z\", \"level\": \"info\", \"event\": \"API keys loaded\"}\n",
      "{\"config_keys\": [\"embedding_model\", \"retriever\", \"llm\"], \"timestamp\": \"2026-01-17T05:45:42.085522Z\", \"level\": \"info\", \"event\": \"YAML config loaded\"}\n",
      "{\"session_id\": \"session_20260117_104542_551b3de3\", \"temp_dir\": \"data\\\\session_20260117_104542_551b3de3\", \"faiss_dir\": \"faiss_index\\\\session_20260117_104542_551b3de3\", \"sessionized\": true, \"timestamp\": \"2026-01-17T05:45:42.087291Z\", \"level\": \"info\", \"event\": \"ChatIngestor initialized\"}\n",
      "{\"uploaded\": \"Physics 9th Ch 1 Final.pdf\", \"saved_as\": \"data\\\\session_20260117_104542_551b3de3\\\\f3496d37.pdf\", \"timestamp\": \"2026-01-17T05:45:42.089831Z\", \"level\": \"info\", \"event\": \"File saved for ingestion\"}\n",
      "{\"count\": 23, \"timestamp\": \"2026-01-17T05:45:43.223807Z\", \"level\": \"info\", \"event\": \"Documents loaded\"}\n",
      "{\"chunks\": 62, \"chunk_size\": 1000, \"overlap\": 200, \"timestamp\": \"2026-01-17T05:45:43.225151Z\", \"level\": \"info\", \"event\": \"Documents split\"}\n",
      "{\"model\": \"models/text-embedding-004\", \"timestamp\": \"2026-01-17T05:45:43.229672Z\", \"level\": \"info\", \"event\": \"Loading embedding model\"}\n",
      "Loading faiss with AVX2 support.\n",
      "Successfully loaded faiss with AVX2 support.\n",
      "{\"added\": 1, \"index\": \"faiss_index\\\\session_20260117_104542_551b3de3\", \"timestamp\": \"2026-01-17T05:45:46.473774Z\", \"level\": \"info\", \"event\": \"FAISS index updated\"}\n",
      "{\"k\": 5, \"fetch_k\": 20, \"lambda_mult\": 0.5, \"timestamp\": \"2026-01-17T05:45:46.475788Z\", \"level\": \"info\", \"event\": \"Using MMR search\"}\n",
      "{\"timestamp\": \"2026-01-17T05:45:46.481825Z\", \"level\": \"info\", \"event\": \"Running in LOCAL mode: .env loaded\"}\n",
      "{\"timestamp\": \"2026-01-17T05:45:46.484847Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from individual env var\"}\n",
      "{\"timestamp\": \"2026-01-17T05:45:46.485928Z\", \"level\": \"info\", \"event\": \"Loaded GOOGLE_API_KEY from individual env var\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_Kv...\", \"GOOGLE_API_KEY\": \"AIzaSy...\"}, \"timestamp\": \"2026-01-17T05:45:46.485928Z\", \"level\": \"info\", \"event\": \"API keys loaded\"}\n",
      "{\"config_keys\": [\"embedding_model\", \"retriever\", \"llm\"], \"timestamp\": \"2026-01-17T05:45:46.495209Z\", \"level\": \"info\", \"event\": \"YAML config loaded\"}\n",
      "{\"provider\": \"google\", \"model\": \"gemini-2.0-flash\", \"timestamp\": \"2026-01-17T05:45:46.497221Z\", \"level\": \"info\", \"event\": \"Loading LLM\"}\n",
      "{\"session_id\": \"session_20260117_104542_551b3de3\", \"timestamp\": \"2026-01-17T05:45:46.508266Z\", \"level\": \"info\", \"event\": \"LLM loaded successfully\"}\n",
      "{\"session_id\": \"session_20260117_104542_551b3de3\", \"timestamp\": \"2026-01-17T05:45:46.510782Z\", \"level\": \"info\", \"event\": \"ConversationalRAG initialized\"}\n",
      "{\"timestamp\": \"2026-01-17T05:45:46.510782Z\", \"level\": \"info\", \"event\": \"Running in LOCAL mode: .env loaded\"}\n",
      "{\"timestamp\": \"2026-01-17T05:45:46.515820Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from individual env var\"}\n",
      "{\"timestamp\": \"2026-01-17T05:45:46.515820Z\", \"level\": \"info\", \"event\": \"Loaded GOOGLE_API_KEY from individual env var\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_Kv...\", \"GOOGLE_API_KEY\": \"AIzaSy...\"}, \"timestamp\": \"2026-01-17T05:45:46.515820Z\", \"level\": \"info\", \"event\": \"API keys loaded\"}\n",
      "{\"config_keys\": [\"embedding_model\", \"retriever\", \"llm\"], \"timestamp\": \"2026-01-17T05:45:46.526070Z\", \"level\": \"info\", \"event\": \"YAML config loaded\"}\n",
      "{\"model\": \"models/text-embedding-004\", \"timestamp\": \"2026-01-17T05:45:46.528085Z\", \"level\": \"info\", \"event\": \"Loading embedding model\"}\n",
      "{\"session_id\": \"session_20260117_104542_551b3de3\", \"timestamp\": \"2026-01-17T05:45:46.559059Z\", \"level\": \"info\", \"event\": \"LCEL graph built successfully\"}\n",
      "{\"index_path\": \"faiss_index/session_20260117_104542_551b3de3\", \"index_name\": \"index\", \"search_type\": \"mmr\", \"k\": 5, \"fetch_k\": 20, \"lambda_mult\": 0.5, \"session_id\": \"session_20260117_104542_551b3de3\", \"timestamp\": \"2026-01-17T05:45:46.559059Z\", \"level\": \"info\", \"event\": \"FAISS retriever loaded successfully\"}\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0, model: gemini-2.0-flash\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash\n",
      "Please retry in 13.145197871s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_input_token_count\"\n",
      "  quota_id: \"GenerateContentInputTokensPerModelPerMinute-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "}\n",
      "violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "}\n",
      "violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 13\n",
      "}\n",
      "].\n",
      "{\"error\": \"429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0, model: gemini-2.0-flash\\nPlease retry in 10.941633249s. [links {\\n  description: \\\"Learn more about Gemini API quotas\\\"\\n  url: \\\"https://ai.google.dev/gemini-api/docs/rate-limits\\\"\\n}\\n, violations {\\n  quota_metric: \\\"generativelanguage.googleapis.com/generate_content_free_tier_requests\\\"\\n  quota_id: \\\"GenerateRequestsPerDayPerProjectPerModel-FreeTier\\\"\\n  quota_dimensions {\\n    key: \\\"model\\\"\\n    value: \\\"gemini-2.0-flash\\\"\\n  }\\n  quota_dimensions {\\n    key: \\\"location\\\"\\n    value: \\\"global\\\"\\n  }\\n}\\nviolations {\\n  quota_metric: \\\"generativelanguage.googleapis.com/generate_content_free_tier_requests\\\"\\n  quota_id: \\\"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\\\"\\n  quota_dimensions {\\n    key: \\\"model\\\"\\n    value: \\\"gemini-2.0-flash\\\"\\n  }\\n  quota_dimensions {\\n    key: \\\"location\\\"\\n    value: \\\"global\\\"\\n  }\\n}\\nviolations {\\n  quota_metric: \\\"generativelanguage.googleapis.com/generate_content_free_tier_input_token_count\\\"\\n  quota_id: \\\"GenerateContentInputTokensPerModelPerMinute-FreeTier\\\"\\n  quota_dimensions {\\n    key: \\\"model\\\"\\n    value: \\\"gemini-2.0-flash\\\"\\n  }\\n  quota_dimensions {\\n    key: \\\"location\\\"\\n    value: \\\"global\\\"\\n  }\\n}\\n, retry_delay {\\n  seconds: 10\\n}\\n]\", \"timestamp\": \"2026-01-17T05:45:50.166389Z\", \"level\": \"error\", \"event\": \"Failed to invoke ConversationalRAG\"}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question: What is the name of the first chapter in this pdf?\n",
      "answer: Error: Error in [d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\google\\api_core\\grpc_helpers.py] at line [77] | Message: Invocation error in ConversationalRAG\n",
      "Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Machine learning\\llmops_series\\src\\document_chat\\retrieval.py\", line 129, in invoke\n",
      "    answer = self.chain.invoke(payload)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py\", line 3044, in invoke\n",
      "    input_ = context.run(step.invoke, input_, config, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py\", line 3773, in invoke\n",
      "    output = {key: future.result() for key, future in zip(steps, futures)}\n",
      "                   ^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Program Files\\Python312\\Lib\\concurrent\\futures\\_base.py\", line 456, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Program Files\\Python312\\Lib\\concurrent\\futures\\_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"C:\\Program Files\\Python312\\Lib\\concurrent\\futures\\thread.py\", line 59, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py\", line 3757, in _invoke_step\n",
      "    return context.run(\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py\", line 3046, in invoke\n",
      "    input_ = context.run(step.invoke, input_, config)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 395, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 980, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 799, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 1045, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py\", line 964, in _generate\n",
      "    response: GenerateContentResponse = _chat_with_retry(\n",
      "                                        ^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py\", line 204, in _chat_with_retry\n",
      "    return _chat_with_retry(**kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\tenacity\\__init__.py\", line 338, in wrapped_f\n",
      "    return copy(f, *args, **kw)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\tenacity\\__init__.py\", line 477, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\tenacity\\__init__.py\", line 378, in iter\n",
      "    result = action(retry_state)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\tenacity\\__init__.py\", line 420, in exc_check\n",
      "    raise retry_exc.reraise()\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\tenacity\\__init__.py\", line 187, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Program Files\\Python312\\Lib\\concurrent\\futures\\_base.py\", line 449, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Program Files\\Python312\\Lib\\concurrent\\futures\\_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\tenacity\\__init__.py\", line 480, in __call__\n",
      "    result = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py\", line 202, in _chat_with_retry\n",
      "    raise e\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py\", line 186, in _chat_with_retry\n",
      "    return generation_method(**kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\google\\ai\\generativelanguage_v1beta\\services\\generative_service\\client.py\", line 868, in generate_content\n",
      "    response = rpc(\n",
      "               ^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\google\\api_core\\gapic_v1\\method.py\", line 131, in __call__\n",
      "    return wrapped_func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py\", line 294, in retry_wrapped_func\n",
      "    return retry_target(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py\", line 156, in retry_target\n",
      "    next_sleep = _retry_error_helper(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\google\\api_core\\retry\\retry_base.py\", line 214, in _retry_error_helper\n",
      "    raise final_exc from source_exc\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py\", line 147, in retry_target\n",
      "    result = target()\n",
      "             ^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\google\\api_core\\timeout.py\", line 130, in func_with_timeout\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\google\\api_core\\grpc_helpers.py\", line 77, in error_remapped_callable\n",
      "    raise exceptions.from_grpc_error(exc) from exc\n",
      "google.api_core.exceptions.ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0, model: gemini-2.0-flash\n",
      "Please retry in 10.941633249s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "}\n",
      "violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "}\n",
      "violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_input_token_count\"\n",
      "  quota_id: \"GenerateContentInputTokensPerModelPerMinute-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 10\n",
      "}\n",
      "]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test the function \n",
    "test_input = {\"question\": \"What is the name of the first chapter in this pdf?\"}\n",
    "result = answer_ai_report_question(test_input)\n",
    "print(\"question:\", test_input[\"question\"])\n",
    "print(\"answer:\", result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9530fd07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"timestamp\": \"2026-01-17T05:45:50.188612Z\", \"level\": \"info\", \"event\": \"Running in LOCAL mode: .env loaded\"}\n",
      "{\"timestamp\": \"2026-01-17T05:45:50.188612Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from individual env var\"}\n",
      "{\"timestamp\": \"2026-01-17T05:45:50.188612Z\", \"level\": \"info\", \"event\": \"Loaded GOOGLE_API_KEY from individual env var\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_Kv...\", \"GOOGLE_API_KEY\": \"AIzaSy...\"}, \"timestamp\": \"2026-01-17T05:45:50.188612Z\", \"level\": \"info\", \"event\": \"API keys loaded\"}\n",
      "{\"config_keys\": [\"embedding_model\", \"retriever\", \"llm\"], \"timestamp\": \"2026-01-17T05:45:50.200751Z\", \"level\": \"info\", \"event\": \"YAML config loaded\"}\n",
      "{\"session_id\": \"session_20260117_104550_700def14\", \"temp_dir\": \"data\\\\session_20260117_104550_700def14\", \"faiss_dir\": \"faiss_index\\\\session_20260117_104550_700def14\", \"sessionized\": true, \"timestamp\": \"2026-01-17T05:45:50.203452Z\", \"level\": \"info\", \"event\": \"ChatIngestor initialized\"}\n",
      "{\"uploaded\": \"Physics 9th Ch 1 Final.pdf\", \"saved_as\": \"data\\\\session_20260117_104550_700def14\\\\eb1097c7.pdf\", \"timestamp\": \"2026-01-17T05:45:50.205077Z\", \"level\": \"info\", \"event\": \"File saved for ingestion\"}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing all questions from the dataset:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"count\": 23, \"timestamp\": \"2026-01-17T05:45:51.551109Z\", \"level\": \"info\", \"event\": \"Documents loaded\"}\n",
      "{\"chunks\": 62, \"chunk_size\": 1000, \"overlap\": 200, \"timestamp\": \"2026-01-17T05:45:51.559629Z\", \"level\": \"info\", \"event\": \"Documents split\"}\n",
      "{\"model\": \"models/text-embedding-004\", \"timestamp\": \"2026-01-17T05:45:51.564976Z\", \"level\": \"info\", \"event\": \"Loading embedding model\"}\n",
      "{\"added\": 1, \"index\": \"faiss_index\\\\session_20260117_104550_700def14\", \"timestamp\": \"2026-01-17T05:45:55.245595Z\", \"level\": \"info\", \"event\": \"FAISS index updated\"}\n",
      "{\"k\": 5, \"fetch_k\": 20, \"lambda_mult\": 0.5, \"timestamp\": \"2026-01-17T05:45:55.245595Z\", \"level\": \"info\", \"event\": \"Using MMR search\"}\n",
      "{\"timestamp\": \"2026-01-17T05:45:55.250914Z\", \"level\": \"info\", \"event\": \"Running in LOCAL mode: .env loaded\"}\n",
      "{\"timestamp\": \"2026-01-17T05:45:55.250914Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from individual env var\"}\n",
      "{\"timestamp\": \"2026-01-17T05:45:55.250914Z\", \"level\": \"info\", \"event\": \"Loaded GOOGLE_API_KEY from individual env var\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_Kv...\", \"GOOGLE_API_KEY\": \"AIzaSy...\"}, \"timestamp\": \"2026-01-17T05:45:55.253435Z\", \"level\": \"info\", \"event\": \"API keys loaded\"}\n",
      "{\"config_keys\": [\"embedding_model\", \"retriever\", \"llm\"], \"timestamp\": \"2026-01-17T05:45:55.257497Z\", \"level\": \"info\", \"event\": \"YAML config loaded\"}\n",
      "{\"provider\": \"google\", \"model\": \"gemini-2.0-flash\", \"timestamp\": \"2026-01-17T05:45:55.257497Z\", \"level\": \"info\", \"event\": \"Loading LLM\"}\n",
      "{\"session_id\": \"session_20260117_104550_700def14\", \"timestamp\": \"2026-01-17T05:45:55.263004Z\", \"level\": \"info\", \"event\": \"LLM loaded successfully\"}\n",
      "{\"session_id\": \"session_20260117_104550_700def14\", \"timestamp\": \"2026-01-17T05:45:55.266537Z\", \"level\": \"info\", \"event\": \"ConversationalRAG initialized\"}\n",
      "{\"timestamp\": \"2026-01-17T05:45:55.269617Z\", \"level\": \"info\", \"event\": \"Running in LOCAL mode: .env loaded\"}\n",
      "{\"timestamp\": \"2026-01-17T05:45:55.269617Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from individual env var\"}\n",
      "{\"timestamp\": \"2026-01-17T05:45:55.269617Z\", \"level\": \"info\", \"event\": \"Loaded GOOGLE_API_KEY from individual env var\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_Kv...\", \"GOOGLE_API_KEY\": \"AIzaSy...\"}, \"timestamp\": \"2026-01-17T05:45:55.274049Z\", \"level\": \"info\", \"event\": \"API keys loaded\"}\n",
      "{\"config_keys\": [\"embedding_model\", \"retriever\", \"llm\"], \"timestamp\": \"2026-01-17T05:45:55.274049Z\", \"level\": \"info\", \"event\": \"YAML config loaded\"}\n",
      "{\"model\": \"models/text-embedding-004\", \"timestamp\": \"2026-01-17T05:45:55.274049Z\", \"level\": \"info\", \"event\": \"Loading embedding model\"}\n",
      "{\"session_id\": \"session_20260117_104550_700def14\", \"timestamp\": \"2026-01-17T05:45:55.316608Z\", \"level\": \"info\", \"event\": \"LCEL graph built successfully\"}\n",
      "{\"index_path\": \"faiss_index/session_20260117_104550_700def14\", \"index_name\": \"index\", \"search_type\": \"mmr\", \"k\": 5, \"fetch_k\": 20, \"lambda_mult\": 0.5, \"session_id\": \"session_20260117_104550_700def14\", \"timestamp\": \"2026-01-17T05:45:55.318511Z\", \"level\": \"info\", \"event\": \"FAISS retriever loaded successfully\"}\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0, model: gemini-2.0-flash\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash\n",
      "Please retry in 5.027726769s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_input_token_count\"\n",
      "  quota_id: \"GenerateContentInputTokensPerModelPerMinute-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "}\n",
      "violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "}\n",
      "violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 5\n",
      "}\n",
      "].\n",
      "{\"error\": \"429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0, model: gemini-2.0-flash\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash\\nPlease retry in 2.777506308s. [links {\\n  description: \\\"Learn more about Gemini API quotas\\\"\\n  url: \\\"https://ai.google.dev/gemini-api/docs/rate-limits\\\"\\n}\\n, violations {\\n  quota_metric: \\\"generativelanguage.googleapis.com/generate_content_free_tier_input_token_count\\\"\\n  quota_id: \\\"GenerateContentInputTokensPerModelPerMinute-FreeTier\\\"\\n  quota_dimensions {\\n    key: \\\"model\\\"\\n    value: \\\"gemini-2.0-flash\\\"\\n  }\\n  quota_dimensions {\\n    key: \\\"location\\\"\\n    value: \\\"global\\\"\\n  }\\n}\\nviolations {\\n  quota_metric: \\\"generativelanguage.googleapis.com/generate_content_free_tier_requests\\\"\\n  quota_id: \\\"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\\\"\\n  quota_dimensions {\\n    key: \\\"model\\\"\\n    value: \\\"gemini-2.0-flash\\\"\\n  }\\n  quota_dimensions {\\n    key: \\\"location\\\"\\n    value: \\\"global\\\"\\n  }\\n}\\nviolations {\\n  quota_metric: \\\"generativelanguage.googleapis.com/generate_content_free_tier_requests\\\"\\n  quota_id: \\\"GenerateRequestsPerDayPerProjectPerModel-FreeTier\\\"\\n  quota_dimensions {\\n    key: \\\"model\\\"\\n    value: \\\"gemini-2.0-flash\\\"\\n  }\\n  quota_dimensions {\\n    key: \\\"location\\\"\\n    value: \\\"global\\\"\\n  }\\n}\\n, retry_delay {\\n  seconds: 2\\n}\\n]\", \"timestamp\": \"2026-01-17T05:45:58.122400Z\", \"level\": \"error\", \"event\": \"Failed to invoke ConversationalRAG\"}\n",
      "{\"timestamp\": \"2026-01-17T05:45:58.128551Z\", \"level\": \"info\", \"event\": \"Running in LOCAL mode: .env loaded\"}\n",
      "{\"timestamp\": \"2026-01-17T05:45:58.128551Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from individual env var\"}\n",
      "{\"timestamp\": \"2026-01-17T05:45:58.128551Z\", \"level\": \"info\", \"event\": \"Loaded GOOGLE_API_KEY from individual env var\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_Kv...\", \"GOOGLE_API_KEY\": \"AIzaSy...\"}, \"timestamp\": \"2026-01-17T05:45:58.128551Z\", \"level\": \"info\", \"event\": \"API keys loaded\"}\n",
      "{\"config_keys\": [\"embedding_model\", \"retriever\", \"llm\"], \"timestamp\": \"2026-01-17T05:45:58.135445Z\", \"level\": \"info\", \"event\": \"YAML config loaded\"}\n",
      "{\"session_id\": \"session_20260117_104558_23620208\", \"temp_dir\": \"data\\\\session_20260117_104558_23620208\", \"faiss_dir\": \"faiss_index\\\\session_20260117_104558_23620208\", \"sessionized\": true, \"timestamp\": \"2026-01-17T05:45:58.139807Z\", \"level\": \"info\", \"event\": \"ChatIngestor initialized\"}\n",
      "{\"uploaded\": \"Physics 9th Ch 1 Final.pdf\", \"saved_as\": \"data\\\\session_20260117_104558_23620208\\\\d2acfa19.pdf\", \"timestamp\": \"2026-01-17T05:45:58.141319Z\", \"level\": \"info\", \"event\": \"File saved for ingestion\"}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1: What is the name of the first chapter in this pdf\n",
      "A1: Error: Error in [d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\google\\api_core\\grpc_helpers.py] at line [77] | Message: Invocation error in ConversationalRAG\n",
      "Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Machine learning\\llmops_series\\src\\document_chat\\retrieval.py\", line 129, in invoke\n",
      "    answer = self.chain.invoke(payload)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py\", line 3044, in invoke\n",
      "    input_ = context.run(step.invoke, input_, config, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py\", line 3773, in invoke\n",
      "    output = {key: future.result() for key, future in zip(steps, futures)}\n",
      "                   ^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Program Files\\Python312\\Lib\\concurrent\\futures\\_base.py\", line 456, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Program Files\\Python312\\Lib\\concurrent\\futures\\_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"C:\\Program Files\\Python312\\Lib\\concurrent\\futures\\thread.py\", line 59, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py\", line 3757, in _invoke_step\n",
      "    return context.run(\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py\", line 3046, in invoke\n",
      "    input_ = context.run(step.invoke, input_, config)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 395, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 980, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 799, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 1045, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py\", line 964, in _generate\n",
      "    response: GenerateContentResponse = _chat_with_retry(\n",
      "                                        ^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py\", line 204, in _chat_with_retry\n",
      "    return _chat_with_retry(**kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\tenacity\\__init__.py\", line 338, in wrapped_f\n",
      "    return copy(f, *args, **kw)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\tenacity\\__init__.py\", line 477, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\tenacity\\__init__.py\", line 378, in iter\n",
      "    result = action(retry_state)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\tenacity\\__init__.py\", line 420, in exc_check\n",
      "    raise retry_exc.reraise()\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\tenacity\\__init__.py\", line 187, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Program Files\\Python312\\Lib\\concurrent\\futures\\_base.py\", line 449, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Program Files\\Python312\\Lib\\concurrent\\futures\\_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\tenacity\\__init__.py\", line 480, in __call__\n",
      "    result = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py\", line 202, in _chat_with_retry\n",
      "    raise e\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py\", line 186, in _chat_with_retry\n",
      "    return generation_method(**kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\google\\ai\\generativelanguage_v1beta\\services\\generative_service\\client.py\", line 868, in generate_content\n",
      "    response = rpc(\n",
      "               ^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\google\\api_core\\gapic_v1\\method.py\", line 131, in __call__\n",
      "    return wrapped_func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py\", line 294, in retry_wrapped_func\n",
      "    return retry_target(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py\", line 156, in retry_target\n",
      "    next_sleep = _retry_error_helper(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\google\\api_core\\retry\\retry_base.py\", line 214, in _retry_error_helper\n",
      "    raise final_exc from source_exc\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py\", line 147, in retry_target\n",
      "    result = target()\n",
      "             ^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\google\\api_core\\timeout.py\", line 130, in func_with_timeout\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\google\\api_core\\grpc_helpers.py\", line 77, in error_remapped_callable\n",
      "    raise exceptions.from_grpc_error(exc) from exc\n",
      "google.api_core.exceptions.ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0, model: gemini-2.0-flash\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash\n",
      "Please retry in 2.777506308s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_input_token_count\"\n",
      "  quota_id: \"GenerateContentInputTokensPerModelPerMinute-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "}\n",
      "violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "}\n",
      "violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 2\n",
      "}\n",
      "]\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"count\": 23, \"timestamp\": \"2026-01-17T05:45:59.169716Z\", \"level\": \"info\", \"event\": \"Documents loaded\"}\n",
      "{\"chunks\": 62, \"chunk_size\": 1000, \"overlap\": 200, \"timestamp\": \"2026-01-17T05:45:59.187229Z\", \"level\": \"info\", \"event\": \"Documents split\"}\n",
      "{\"model\": \"models/text-embedding-004\", \"timestamp\": \"2026-01-17T05:45:59.187690Z\", \"level\": \"info\", \"event\": \"Loading embedding model\"}\n",
      "{\"added\": 1, \"index\": \"faiss_index\\\\session_20260117_104558_23620208\", \"timestamp\": \"2026-01-17T05:46:02.361059Z\", \"level\": \"info\", \"event\": \"FAISS index updated\"}\n",
      "{\"k\": 5, \"fetch_k\": 20, \"lambda_mult\": 0.5, \"timestamp\": \"2026-01-17T05:46:02.364379Z\", \"level\": \"info\", \"event\": \"Using MMR search\"}\n",
      "{\"timestamp\": \"2026-01-17T05:46:02.368577Z\", \"level\": \"info\", \"event\": \"Running in LOCAL mode: .env loaded\"}\n",
      "{\"timestamp\": \"2026-01-17T05:46:02.372745Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from individual env var\"}\n",
      "{\"timestamp\": \"2026-01-17T05:46:02.372745Z\", \"level\": \"info\", \"event\": \"Loaded GOOGLE_API_KEY from individual env var\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_Kv...\", \"GOOGLE_API_KEY\": \"AIzaSy...\"}, \"timestamp\": \"2026-01-17T05:46:02.372745Z\", \"level\": \"info\", \"event\": \"API keys loaded\"}\n",
      "{\"config_keys\": [\"embedding_model\", \"retriever\", \"llm\"], \"timestamp\": \"2026-01-17T05:46:02.379003Z\", \"level\": \"info\", \"event\": \"YAML config loaded\"}\n",
      "{\"provider\": \"google\", \"model\": \"gemini-2.0-flash\", \"timestamp\": \"2026-01-17T05:46:02.380574Z\", \"level\": \"info\", \"event\": \"Loading LLM\"}\n",
      "{\"session_id\": \"session_20260117_104558_23620208\", \"timestamp\": \"2026-01-17T05:46:02.381708Z\", \"level\": \"info\", \"event\": \"LLM loaded successfully\"}\n",
      "{\"session_id\": \"session_20260117_104558_23620208\", \"timestamp\": \"2026-01-17T05:46:02.387642Z\", \"level\": \"info\", \"event\": \"ConversationalRAG initialized\"}\n",
      "{\"timestamp\": \"2026-01-17T05:46:02.389949Z\", \"level\": \"info\", \"event\": \"Running in LOCAL mode: .env loaded\"}\n",
      "{\"timestamp\": \"2026-01-17T05:46:02.389949Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from individual env var\"}\n",
      "{\"timestamp\": \"2026-01-17T05:46:02.394059Z\", \"level\": \"info\", \"event\": \"Loaded GOOGLE_API_KEY from individual env var\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_Kv...\", \"GOOGLE_API_KEY\": \"AIzaSy...\"}, \"timestamp\": \"2026-01-17T05:46:02.394059Z\", \"level\": \"info\", \"event\": \"API keys loaded\"}\n",
      "{\"config_keys\": [\"embedding_model\", \"retriever\", \"llm\"], \"timestamp\": \"2026-01-17T05:46:02.396006Z\", \"level\": \"info\", \"event\": \"YAML config loaded\"}\n",
      "{\"model\": \"models/text-embedding-004\", \"timestamp\": \"2026-01-17T05:46:02.396006Z\", \"level\": \"info\", \"event\": \"Loading embedding model\"}\n",
      "{\"session_id\": \"session_20260117_104558_23620208\", \"timestamp\": \"2026-01-17T05:46:02.446346Z\", \"level\": \"info\", \"event\": \"LCEL graph built successfully\"}\n",
      "{\"index_path\": \"faiss_index/session_20260117_104558_23620208\", \"index_name\": \"index\", \"search_type\": \"mmr\", \"k\": 5, \"fetch_k\": 20, \"lambda_mult\": 0.5, \"session_id\": \"session_20260117_104558_23620208\", \"timestamp\": \"2026-01-17T05:46:02.453200Z\", \"level\": \"info\", \"event\": \"FAISS retriever loaded successfully\"}\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0, model: gemini-2.0-flash\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash\n",
      "Please retry in 57.281959966s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_input_token_count\"\n",
      "  quota_id: \"GenerateContentInputTokensPerModelPerMinute-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "}\n",
      "violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "}\n",
      "violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 57\n",
      "}\n",
      "].\n",
      "{\"error\": \"429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0, model: gemini-2.0-flash\\nPlease retry in 55.078343416s. [links {\\n  description: \\\"Learn more about Gemini API quotas\\\"\\n  url: \\\"https://ai.google.dev/gemini-api/docs/rate-limits\\\"\\n}\\n, violations {\\n  quota_metric: \\\"generativelanguage.googleapis.com/generate_content_free_tier_requests\\\"\\n  quota_id: \\\"GenerateRequestsPerDayPerProjectPerModel-FreeTier\\\"\\n  quota_dimensions {\\n    key: \\\"model\\\"\\n    value: \\\"gemini-2.0-flash\\\"\\n  }\\n  quota_dimensions {\\n    key: \\\"location\\\"\\n    value: \\\"global\\\"\\n  }\\n}\\nviolations {\\n  quota_metric: \\\"generativelanguage.googleapis.com/generate_content_free_tier_requests\\\"\\n  quota_id: \\\"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\\\"\\n  quota_dimensions {\\n    key: \\\"model\\\"\\n    value: \\\"gemini-2.0-flash\\\"\\n  }\\n  quota_dimensions {\\n    key: \\\"location\\\"\\n    value: \\\"global\\\"\\n  }\\n}\\nviolations {\\n  quota_metric: \\\"generativelanguage.googleapis.com/generate_content_free_tier_input_token_count\\\"\\n  quota_id: \\\"GenerateContentInputTokensPerModelPerMinute-FreeTier\\\"\\n  quota_dimensions {\\n    key: \\\"model\\\"\\n    value: \\\"gemini-2.0-flash\\\"\\n  }\\n  quota_dimensions {\\n    key: \\\"location\\\"\\n    value: \\\"global\\\"\\n  }\\n}\\n, retry_delay {\\n  seconds: 55\\n}\\n]\", \"timestamp\": \"2026-01-17T05:46:05.853134Z\", \"level\": \"error\", \"event\": \"Failed to invoke ConversationalRAG\"}\n",
      "{\"timestamp\": \"2026-01-17T05:46:05.856856Z\", \"level\": \"info\", \"event\": \"Running in LOCAL mode: .env loaded\"}\n",
      "{\"timestamp\": \"2026-01-17T05:46:05.856856Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from individual env var\"}\n",
      "{\"timestamp\": \"2026-01-17T05:46:05.856856Z\", \"level\": \"info\", \"event\": \"Loaded GOOGLE_API_KEY from individual env var\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_Kv...\", \"GOOGLE_API_KEY\": \"AIzaSy...\"}, \"timestamp\": \"2026-01-17T05:46:05.861453Z\", \"level\": \"info\", \"event\": \"API keys loaded\"}\n",
      "{\"config_keys\": [\"embedding_model\", \"retriever\", \"llm\"], \"timestamp\": \"2026-01-17T05:46:05.864380Z\", \"level\": \"info\", \"event\": \"YAML config loaded\"}\n",
      "{\"session_id\": \"session_20260117_104605_98c202cb\", \"temp_dir\": \"data\\\\session_20260117_104605_98c202cb\", \"faiss_dir\": \"faiss_index\\\\session_20260117_104605_98c202cb\", \"sessionized\": true, \"timestamp\": \"2026-01-17T05:46:05.865387Z\", \"level\": \"info\", \"event\": \"ChatIngestor initialized\"}\n",
      "{\"uploaded\": \"Physics 9th Ch 1 Final.pdf\", \"saved_as\": \"data\\\\session_20260117_104605_98c202cb\\\\16414a4e.pdf\", \"timestamp\": \"2026-01-17T05:46:05.869225Z\", \"level\": \"info\", \"event\": \"File saved for ingestion\"}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q2: what is the atomic number of hydrogen?\n",
      "A2: Error: Error in [d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\google\\api_core\\grpc_helpers.py] at line [77] | Message: Invocation error in ConversationalRAG\n",
      "Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Machine learning\\llmops_series\\src\\document_chat\\retrieval.py\", line 129, in invoke\n",
      "    answer = self.chain.invoke(payload)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py\", line 3044, in invoke\n",
      "    input_ = context.run(step.invoke, input_, config, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py\", line 3773, in invoke\n",
      "    output = {key: future.result() for key, future in zip(steps, futures)}\n",
      "                   ^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Program Files\\Python312\\Lib\\concurrent\\futures\\_base.py\", line 456, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Program Files\\Python312\\Lib\\concurrent\\futures\\_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"C:\\Program Files\\Python312\\Lib\\concurrent\\futures\\thread.py\", line 59, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py\", line 3757, in _invoke_step\n",
      "    return context.run(\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py\", line 3046, in invoke\n",
      "    input_ = context.run(step.invoke, input_, config)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 395, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 980, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 799, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 1045, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py\", line 964, in _generate\n",
      "    response: GenerateContentResponse = _chat_with_retry(\n",
      "                                        ^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py\", line 204, in _chat_with_retry\n",
      "    return _chat_with_retry(**kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\tenacity\\__init__.py\", line 338, in wrapped_f\n",
      "    return copy(f, *args, **kw)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\tenacity\\__init__.py\", line 477, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\tenacity\\__init__.py\", line 378, in iter\n",
      "    result = action(retry_state)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\tenacity\\__init__.py\", line 420, in exc_check\n",
      "    raise retry_exc.reraise()\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\tenacity\\__init__.py\", line 187, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Program Files\\Python312\\Lib\\concurrent\\futures\\_base.py\", line 449, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Program Files\\Python312\\Lib\\concurrent\\futures\\_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\tenacity\\__init__.py\", line 480, in __call__\n",
      "    result = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py\", line 202, in _chat_with_retry\n",
      "    raise e\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py\", line 186, in _chat_with_retry\n",
      "    return generation_method(**kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\google\\ai\\generativelanguage_v1beta\\services\\generative_service\\client.py\", line 868, in generate_content\n",
      "    response = rpc(\n",
      "               ^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\google\\api_core\\gapic_v1\\method.py\", line 131, in __call__\n",
      "    return wrapped_func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py\", line 294, in retry_wrapped_func\n",
      "    return retry_target(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py\", line 156, in retry_target\n",
      "    next_sleep = _retry_error_helper(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\google\\api_core\\retry\\retry_base.py\", line 214, in _retry_error_helper\n",
      "    raise final_exc from source_exc\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py\", line 147, in retry_target\n",
      "    result = target()\n",
      "             ^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\google\\api_core\\timeout.py\", line 130, in func_with_timeout\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\google\\api_core\\grpc_helpers.py\", line 77, in error_remapped_callable\n",
      "    raise exceptions.from_grpc_error(exc) from exc\n",
      "google.api_core.exceptions.ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0, model: gemini-2.0-flash\n",
      "Please retry in 55.078343416s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "}\n",
      "violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "}\n",
      "violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_input_token_count\"\n",
      "  quota_id: \"GenerateContentInputTokensPerModelPerMinute-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 55\n",
      "}\n",
      "]\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"count\": 23, \"timestamp\": \"2026-01-17T05:46:06.970075Z\", \"level\": \"info\", \"event\": \"Documents loaded\"}\n",
      "{\"chunks\": 62, \"chunk_size\": 1000, \"overlap\": 200, \"timestamp\": \"2026-01-17T05:46:06.973863Z\", \"level\": \"info\", \"event\": \"Documents split\"}\n",
      "{\"model\": \"models/text-embedding-004\", \"timestamp\": \"2026-01-17T05:46:06.978544Z\", \"level\": \"info\", \"event\": \"Loading embedding model\"}\n",
      "{\"added\": 1, \"index\": \"faiss_index\\\\session_20260117_104605_98c202cb\", \"timestamp\": \"2026-01-17T05:46:10.087621Z\", \"level\": \"info\", \"event\": \"FAISS index updated\"}\n",
      "{\"k\": 5, \"fetch_k\": 20, \"lambda_mult\": 0.5, \"timestamp\": \"2026-01-17T05:46:10.089921Z\", \"level\": \"info\", \"event\": \"Using MMR search\"}\n",
      "{\"timestamp\": \"2026-01-17T05:46:10.099870Z\", \"level\": \"info\", \"event\": \"Running in LOCAL mode: .env loaded\"}\n",
      "{\"timestamp\": \"2026-01-17T05:46:10.104282Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from individual env var\"}\n",
      "{\"timestamp\": \"2026-01-17T05:46:10.107892Z\", \"level\": \"info\", \"event\": \"Loaded GOOGLE_API_KEY from individual env var\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_Kv...\", \"GOOGLE_API_KEY\": \"AIzaSy...\"}, \"timestamp\": \"2026-01-17T05:46:10.109158Z\", \"level\": \"info\", \"event\": \"API keys loaded\"}\n",
      "{\"config_keys\": [\"embedding_model\", \"retriever\", \"llm\"], \"timestamp\": \"2026-01-17T05:46:10.118943Z\", \"level\": \"info\", \"event\": \"YAML config loaded\"}\n",
      "{\"provider\": \"google\", \"model\": \"gemini-2.0-flash\", \"timestamp\": \"2026-01-17T05:46:10.122817Z\", \"level\": \"info\", \"event\": \"Loading LLM\"}\n",
      "{\"session_id\": \"session_20260117_104605_98c202cb\", \"timestamp\": \"2026-01-17T05:46:10.135751Z\", \"level\": \"info\", \"event\": \"LLM loaded successfully\"}\n",
      "{\"session_id\": \"session_20260117_104605_98c202cb\", \"timestamp\": \"2026-01-17T05:46:10.139454Z\", \"level\": \"info\", \"event\": \"ConversationalRAG initialized\"}\n",
      "{\"timestamp\": \"2026-01-17T05:46:10.145200Z\", \"level\": \"info\", \"event\": \"Running in LOCAL mode: .env loaded\"}\n",
      "{\"timestamp\": \"2026-01-17T05:46:10.148607Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from individual env var\"}\n",
      "{\"timestamp\": \"2026-01-17T05:46:10.148607Z\", \"level\": \"info\", \"event\": \"Loaded GOOGLE_API_KEY from individual env var\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_Kv...\", \"GOOGLE_API_KEY\": \"AIzaSy...\"}, \"timestamp\": \"2026-01-17T05:46:10.152166Z\", \"level\": \"info\", \"event\": \"API keys loaded\"}\n",
      "{\"config_keys\": [\"embedding_model\", \"retriever\", \"llm\"], \"timestamp\": \"2026-01-17T05:46:10.166072Z\", \"level\": \"info\", \"event\": \"YAML config loaded\"}\n",
      "{\"model\": \"models/text-embedding-004\", \"timestamp\": \"2026-01-17T05:46:10.169526Z\", \"level\": \"info\", \"event\": \"Loading embedding model\"}\n",
      "{\"session_id\": \"session_20260117_104605_98c202cb\", \"timestamp\": \"2026-01-17T05:46:10.239235Z\", \"level\": \"info\", \"event\": \"LCEL graph built successfully\"}\n",
      "{\"index_path\": \"faiss_index/session_20260117_104605_98c202cb\", \"index_name\": \"index\", \"search_type\": \"mmr\", \"k\": 5, \"fetch_k\": 20, \"lambda_mult\": 0.5, \"session_id\": \"session_20260117_104605_98c202cb\", \"timestamp\": \"2026-01-17T05:46:10.241875Z\", \"level\": \"info\", \"event\": \"FAISS retriever loaded successfully\"}\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0, model: gemini-2.0-flash\n",
      "Please retry in 50.081178449s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "}\n",
      "violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "}\n",
      "violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_input_token_count\"\n",
      "  quota_id: \"GenerateContentInputTokensPerModelPerMinute-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 50\n",
      "}\n",
      "].\n",
      "{\"error\": \"429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0, model: gemini-2.0-flash\\nPlease retry in 47.892248479s. [links {\\n  description: \\\"Learn more about Gemini API quotas\\\"\\n  url: \\\"https://ai.google.dev/gemini-api/docs/rate-limits\\\"\\n}\\n, violations {\\n  quota_metric: \\\"generativelanguage.googleapis.com/generate_content_free_tier_requests\\\"\\n  quota_id: \\\"GenerateRequestsPerDayPerProjectPerModel-FreeTier\\\"\\n  quota_dimensions {\\n    key: \\\"model\\\"\\n    value: \\\"gemini-2.0-flash\\\"\\n  }\\n  quota_dimensions {\\n    key: \\\"location\\\"\\n    value: \\\"global\\\"\\n  }\\n}\\nviolations {\\n  quota_metric: \\\"generativelanguage.googleapis.com/generate_content_free_tier_requests\\\"\\n  quota_id: \\\"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\\\"\\n  quota_dimensions {\\n    key: \\\"model\\\"\\n    value: \\\"gemini-2.0-flash\\\"\\n  }\\n  quota_dimensions {\\n    key: \\\"location\\\"\\n    value: \\\"global\\\"\\n  }\\n}\\nviolations {\\n  quota_metric: \\\"generativelanguage.googleapis.com/generate_content_free_tier_input_token_count\\\"\\n  quota_id: \\\"GenerateContentInputTokensPerModelPerMinute-FreeTier\\\"\\n  quota_dimensions {\\n    key: \\\"model\\\"\\n    value: \\\"gemini-2.0-flash\\\"\\n  }\\n  quota_dimensions {\\n    key: \\\"location\\\"\\n    value: \\\"global\\\"\\n  }\\n}\\n, retry_delay {\\n  seconds: 47\\n}\\n]\", \"timestamp\": \"2026-01-17T05:46:13.020350Z\", \"level\": \"error\", \"event\": \"Failed to invoke ConversationalRAG\"}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q3: Total number of chapters in this document?\n",
      "A3: Error: Error in [d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\google\\api_core\\grpc_helpers.py] at line [77] | Message: Invocation error in ConversationalRAG\n",
      "Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Machine learning\\llmops_series\\src\\document_chat\\retrieval.py\", line 129, in invoke\n",
      "    answer = self.chain.invoke(payload)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py\", line 3044, in invoke\n",
      "    input_ = context.run(step.invoke, input_, config, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py\", line 3773, in invoke\n",
      "    output = {key: future.result() for key, future in zip(steps, futures)}\n",
      "                   ^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Program Files\\Python312\\Lib\\concurrent\\futures\\_base.py\", line 456, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Program Files\\Python312\\Lib\\concurrent\\futures\\_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"C:\\Program Files\\Python312\\Lib\\concurrent\\futures\\thread.py\", line 59, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py\", line 3757, in _invoke_step\n",
      "    return context.run(\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py\", line 3046, in invoke\n",
      "    input_ = context.run(step.invoke, input_, config)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 395, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 980, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 799, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 1045, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py\", line 964, in _generate\n",
      "    response: GenerateContentResponse = _chat_with_retry(\n",
      "                                        ^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py\", line 204, in _chat_with_retry\n",
      "    return _chat_with_retry(**kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\tenacity\\__init__.py\", line 338, in wrapped_f\n",
      "    return copy(f, *args, **kw)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\tenacity\\__init__.py\", line 477, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\tenacity\\__init__.py\", line 378, in iter\n",
      "    result = action(retry_state)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\tenacity\\__init__.py\", line 420, in exc_check\n",
      "    raise retry_exc.reraise()\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\tenacity\\__init__.py\", line 187, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Program Files\\Python312\\Lib\\concurrent\\futures\\_base.py\", line 449, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Program Files\\Python312\\Lib\\concurrent\\futures\\_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\tenacity\\__init__.py\", line 480, in __call__\n",
      "    result = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py\", line 202, in _chat_with_retry\n",
      "    raise e\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py\", line 186, in _chat_with_retry\n",
      "    return generation_method(**kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\google\\ai\\generativelanguage_v1beta\\services\\generative_service\\client.py\", line 868, in generate_content\n",
      "    response = rpc(\n",
      "               ^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\google\\api_core\\gapic_v1\\method.py\", line 131, in __call__\n",
      "    return wrapped_func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py\", line 294, in retry_wrapped_func\n",
      "    return retry_target(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py\", line 156, in retry_target\n",
      "    next_sleep = _retry_error_helper(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\google\\api_core\\retry\\retry_base.py\", line 214, in _retry_error_helper\n",
      "    raise final_exc from source_exc\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py\", line 147, in retry_target\n",
      "    result = target()\n",
      "             ^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\google\\api_core\\timeout.py\", line 130, in func_with_timeout\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\google\\api_core\\grpc_helpers.py\", line 77, in error_remapped_callable\n",
      "    raise exceptions.from_grpc_error(exc) from exc\n",
      "google.api_core.exceptions.ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0, model: gemini-2.0-flash\n",
      "Please retry in 47.892248479s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "}\n",
      "violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "}\n",
      "violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_input_token_count\"\n",
      "  quota_id: \"GenerateContentInputTokensPerModelPerMinute-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 47\n",
      "}\n",
      "]\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example : Test with all golden questions\n",
    "print(\"Testing all questions from the dataset:\\n\")\n",
    "for i, q in enumerate(inputs, 1):\n",
    "    test_input = {\"question\": q}\n",
    "    result = answer_ai_report_question(test_input)\n",
    "    print(f\"Q{i}: {q}\")\n",
    "    print(f\"A{i}: {result['answer']}\\n\")\n",
    "    print(\"-\" * 80 + \"\\n\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "547151a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'test-llmops_dataset-qa-rag-32385ac8' at:\n",
      "https://smith.langchain.com/o/e8008ce9-eb06-5861-b8e8-97dd6daeac49/datasets/60a4e1ba-caf7-4cd7-9450-04c89b74c7fa/compare?selectedSessions=5e3e15ad-7db9-4d9b-8cc8-8e9ed1d3e8b1\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]{\"timestamp\": \"2026-01-17T05:46:19.033968Z\", \"level\": \"info\", \"event\": \"Running in LOCAL mode: .env loaded\"}\n",
      "{\"timestamp\": \"2026-01-17T05:46:19.033968Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from individual env var\"}\n",
      "{\"timestamp\": \"2026-01-17T05:46:19.033968Z\", \"level\": \"info\", \"event\": \"Loaded GOOGLE_API_KEY from individual env var\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_Kv...\", \"GOOGLE_API_KEY\": \"AIzaSy...\"}, \"timestamp\": \"2026-01-17T05:46:19.043127Z\", \"level\": \"info\", \"event\": \"API keys loaded\"}\n",
      "{\"config_keys\": [\"embedding_model\", \"retriever\", \"llm\"], \"timestamp\": \"2026-01-17T05:46:19.044569Z\", \"level\": \"info\", \"event\": \"YAML config loaded\"}\n",
      "{\"session_id\": \"session_20260117_104619_0ead42a2\", \"temp_dir\": \"data\\\\session_20260117_104619_0ead42a2\", \"faiss_dir\": \"faiss_index\\\\session_20260117_104619_0ead42a2\", \"sessionized\": true, \"timestamp\": \"2026-01-17T05:46:19.049859Z\", \"level\": \"info\", \"event\": \"ChatIngestor initialized\"}\n",
      "{\"uploaded\": \"Physics 9th Ch 1 Final.pdf\", \"saved_as\": \"data\\\\session_20260117_104619_0ead42a2\\\\9403e7b5.pdf\", \"timestamp\": \"2026-01-17T05:46:19.052900Z\", \"level\": \"info\", \"event\": \"File saved for ingestion\"}\n",
      "{\"count\": 23, \"timestamp\": \"2026-01-17T05:46:19.887197Z\", \"level\": \"info\", \"event\": \"Documents loaded\"}\n",
      "{\"chunks\": 62, \"chunk_size\": 1000, \"overlap\": 200, \"timestamp\": \"2026-01-17T05:46:19.896057Z\", \"level\": \"info\", \"event\": \"Documents split\"}\n",
      "{\"model\": \"models/text-embedding-004\", \"timestamp\": \"2026-01-17T05:46:19.896057Z\", \"level\": \"info\", \"event\": \"Loading embedding model\"}\n",
      "{\"added\": 1, \"index\": \"faiss_index\\\\session_20260117_104619_0ead42a2\", \"timestamp\": \"2026-01-17T05:46:23.592142Z\", \"level\": \"info\", \"event\": \"FAISS index updated\"}\n",
      "{\"k\": 5, \"fetch_k\": 20, \"lambda_mult\": 0.5, \"timestamp\": \"2026-01-17T05:46:23.593638Z\", \"level\": \"info\", \"event\": \"Using MMR search\"}\n",
      "{\"timestamp\": \"2026-01-17T05:46:23.597060Z\", \"level\": \"info\", \"event\": \"Running in LOCAL mode: .env loaded\"}\n",
      "{\"timestamp\": \"2026-01-17T05:46:23.599074Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from individual env var\"}\n",
      "{\"timestamp\": \"2026-01-17T05:46:23.599074Z\", \"level\": \"info\", \"event\": \"Loaded GOOGLE_API_KEY from individual env var\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_Kv...\", \"GOOGLE_API_KEY\": \"AIzaSy...\"}, \"timestamp\": \"2026-01-17T05:46:23.602571Z\", \"level\": \"info\", \"event\": \"API keys loaded\"}\n",
      "{\"config_keys\": [\"embedding_model\", \"retriever\", \"llm\"], \"timestamp\": \"2026-01-17T05:46:23.606128Z\", \"level\": \"info\", \"event\": \"YAML config loaded\"}\n",
      "{\"provider\": \"google\", \"model\": \"gemini-2.0-flash\", \"timestamp\": \"2026-01-17T05:46:23.606128Z\", \"level\": \"info\", \"event\": \"Loading LLM\"}\n",
      "{\"session_id\": \"session_20260117_104619_0ead42a2\", \"timestamp\": \"2026-01-17T05:46:23.612122Z\", \"level\": \"info\", \"event\": \"LLM loaded successfully\"}\n",
      "{\"session_id\": \"session_20260117_104619_0ead42a2\", \"timestamp\": \"2026-01-17T05:46:23.613673Z\", \"level\": \"info\", \"event\": \"ConversationalRAG initialized\"}\n",
      "{\"timestamp\": \"2026-01-17T05:46:23.615354Z\", \"level\": \"info\", \"event\": \"Running in LOCAL mode: .env loaded\"}\n",
      "{\"timestamp\": \"2026-01-17T05:46:23.619102Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from individual env var\"}\n",
      "{\"timestamp\": \"2026-01-17T05:46:23.619102Z\", \"level\": \"info\", \"event\": \"Loaded GOOGLE_API_KEY from individual env var\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_Kv...\", \"GOOGLE_API_KEY\": \"AIzaSy...\"}, \"timestamp\": \"2026-01-17T05:46:23.622163Z\", \"level\": \"info\", \"event\": \"API keys loaded\"}\n",
      "{\"config_keys\": [\"embedding_model\", \"retriever\", \"llm\"], \"timestamp\": \"2026-01-17T05:46:23.622163Z\", \"level\": \"info\", \"event\": \"YAML config loaded\"}\n",
      "{\"model\": \"models/text-embedding-004\", \"timestamp\": \"2026-01-17T05:46:23.622163Z\", \"level\": \"info\", \"event\": \"Loading embedding model\"}\n",
      "{\"session_id\": \"session_20260117_104619_0ead42a2\", \"timestamp\": \"2026-01-17T05:46:23.668740Z\", \"level\": \"info\", \"event\": \"LCEL graph built successfully\"}\n",
      "{\"index_path\": \"faiss_index/session_20260117_104619_0ead42a2\", \"index_name\": \"index\", \"search_type\": \"mmr\", \"k\": 5, \"fetch_k\": 20, \"lambda_mult\": 0.5, \"session_id\": \"session_20260117_104619_0ead42a2\", \"timestamp\": \"2026-01-17T05:46:23.668740Z\", \"level\": \"info\", \"event\": \"FAISS retriever loaded successfully\"}\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0, model: gemini-2.0-flash\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash\n",
      "Please retry in 36.673145062s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_input_token_count\"\n",
      "  quota_id: \"GenerateContentInputTokensPerModelPerMinute-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "}\n",
      "violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "}\n",
      "violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 36\n",
      "}\n",
      "].\n",
      "{\"error\": \"429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0, model: gemini-2.0-flash\\nPlease retry in 34.486088209s. [links {\\n  description: \\\"Learn more about Gemini API quotas\\\"\\n  url: \\\"https://ai.google.dev/gemini-api/docs/rate-limits\\\"\\n}\\n, violations {\\n  quota_metric: \\\"generativelanguage.googleapis.com/generate_content_free_tier_requests\\\"\\n  quota_id: \\\"GenerateRequestsPerDayPerProjectPerModel-FreeTier\\\"\\n  quota_dimensions {\\n    key: \\\"model\\\"\\n    value: \\\"gemini-2.0-flash\\\"\\n  }\\n  quota_dimensions {\\n    key: \\\"location\\\"\\n    value: \\\"global\\\"\\n  }\\n}\\nviolations {\\n  quota_metric: \\\"generativelanguage.googleapis.com/generate_content_free_tier_requests\\\"\\n  quota_id: \\\"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\\\"\\n  quota_dimensions {\\n    key: \\\"model\\\"\\n    value: \\\"gemini-2.0-flash\\\"\\n  }\\n  quota_dimensions {\\n    key: \\\"location\\\"\\n    value: \\\"global\\\"\\n  }\\n}\\nviolations {\\n  quota_metric: \\\"generativelanguage.googleapis.com/generate_content_free_tier_input_token_count\\\"\\n  quota_id: \\\"GenerateContentInputTokensPerModelPerMinute-FreeTier\\\"\\n  quota_dimensions {\\n    key: \\\"model\\\"\\n    value: \\\"gemini-2.0-flash\\\"\\n  }\\n  quota_dimensions {\\n    key: \\\"location\\\"\\n    value: \\\"global\\\"\\n  }\\n}\\n, retry_delay {\\n  seconds: 34\\n}\\n]\", \"timestamp\": \"2026-01-17T05:46:26.432191Z\", \"level\": \"error\", \"event\": \"Failed to invoke ConversationalRAG\"}\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 401 Unauthorized\"\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run 0da318a3-ca24-4085-895f-4593fb7a6a11: AuthenticationError(\"Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-********************************************************************************************************************************************************ob0A. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'code': 'invalid_api_key', 'param': None}, 'status': 401}\")\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1619, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 693, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 693, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 266, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 223, in evaluate_strings\n",
      "    return self._evaluate_strings(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\langchain\\evaluation\\qa\\eval_chain.py\", line 313, in _evaluate_strings\n",
      "    result = self(\n",
      "             ^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 189, in warning_emitting_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 410, in __call__\n",
      "    return self.invoke(\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 165, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
      "    response = self.generate([inputs], run_manager=run_manager)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
      "    return self.llm.generate_prompt(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 980, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 799, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 1045, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 689, in _generate\n",
      "    response = self.client.create(**payload)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 286, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 1147, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1259, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1047, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.AuthenticationError: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-********************************************************************************************************************************************************ob0A. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'code': 'invalid_api_key', 'param': None}, 'status': 401}\n",
      "1it [00:08,  8.14s/it]{\"timestamp\": \"2026-01-17T05:46:27.179049Z\", \"level\": \"info\", \"event\": \"Running in LOCAL mode: .env loaded\"}\n",
      "{\"timestamp\": \"2026-01-17T05:46:27.179049Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from individual env var\"}\n",
      "{\"timestamp\": \"2026-01-17T05:46:27.179049Z\", \"level\": \"info\", \"event\": \"Loaded GOOGLE_API_KEY from individual env var\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_Kv...\", \"GOOGLE_API_KEY\": \"AIzaSy...\"}, \"timestamp\": \"2026-01-17T05:46:27.183130Z\", \"level\": \"info\", \"event\": \"API keys loaded\"}\n",
      "{\"config_keys\": [\"embedding_model\", \"retriever\", \"llm\"], \"timestamp\": \"2026-01-17T05:46:27.185634Z\", \"level\": \"info\", \"event\": \"YAML config loaded\"}\n",
      "{\"session_id\": \"session_20260117_104627_3ea02899\", \"temp_dir\": \"data\\\\session_20260117_104627_3ea02899\", \"faiss_dir\": \"faiss_index\\\\session_20260117_104627_3ea02899\", \"sessionized\": true, \"timestamp\": \"2026-01-17T05:46:27.191455Z\", \"level\": \"info\", \"event\": \"ChatIngestor initialized\"}\n",
      "{\"uploaded\": \"Physics 9th Ch 1 Final.pdf\", \"saved_as\": \"data\\\\session_20260117_104627_3ea02899\\\\5deadf42.pdf\", \"timestamp\": \"2026-01-17T05:46:27.194285Z\", \"level\": \"info\", \"event\": \"File saved for ingestion\"}\n",
      "{\"count\": 23, \"timestamp\": \"2026-01-17T05:46:28.421757Z\", \"level\": \"info\", \"event\": \"Documents loaded\"}\n",
      "{\"chunks\": 62, \"chunk_size\": 1000, \"overlap\": 200, \"timestamp\": \"2026-01-17T05:46:28.421757Z\", \"level\": \"info\", \"event\": \"Documents split\"}\n",
      "{\"model\": \"models/text-embedding-004\", \"timestamp\": \"2026-01-17T05:46:28.428424Z\", \"level\": \"info\", \"event\": \"Loading embedding model\"}\n",
      "{\"added\": 1, \"index\": \"faiss_index\\\\session_20260117_104627_3ea02899\", \"timestamp\": \"2026-01-17T05:46:32.142151Z\", \"level\": \"info\", \"event\": \"FAISS index updated\"}\n",
      "{\"k\": 5, \"fetch_k\": 20, \"lambda_mult\": 0.5, \"timestamp\": \"2026-01-17T05:46:32.142151Z\", \"level\": \"info\", \"event\": \"Using MMR search\"}\n",
      "{\"timestamp\": \"2026-01-17T05:46:32.147337Z\", \"level\": \"info\", \"event\": \"Running in LOCAL mode: .env loaded\"}\n",
      "{\"timestamp\": \"2026-01-17T05:46:32.147946Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from individual env var\"}\n",
      "{\"timestamp\": \"2026-01-17T05:46:32.149051Z\", \"level\": \"info\", \"event\": \"Loaded GOOGLE_API_KEY from individual env var\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_Kv...\", \"GOOGLE_API_KEY\": \"AIzaSy...\"}, \"timestamp\": \"2026-01-17T05:46:32.149051Z\", \"level\": \"info\", \"event\": \"API keys loaded\"}\n",
      "{\"config_keys\": [\"embedding_model\", \"retriever\", \"llm\"], \"timestamp\": \"2026-01-17T05:46:32.153613Z\", \"level\": \"info\", \"event\": \"YAML config loaded\"}\n",
      "{\"provider\": \"google\", \"model\": \"gemini-2.0-flash\", \"timestamp\": \"2026-01-17T05:46:32.154929Z\", \"level\": \"info\", \"event\": \"Loading LLM\"}\n",
      "{\"session_id\": \"session_20260117_104627_3ea02899\", \"timestamp\": \"2026-01-17T05:46:32.158608Z\", \"level\": \"info\", \"event\": \"LLM loaded successfully\"}\n",
      "{\"session_id\": \"session_20260117_104627_3ea02899\", \"timestamp\": \"2026-01-17T05:46:32.160089Z\", \"level\": \"info\", \"event\": \"ConversationalRAG initialized\"}\n",
      "{\"timestamp\": \"2026-01-17T05:46:32.162645Z\", \"level\": \"info\", \"event\": \"Running in LOCAL mode: .env loaded\"}\n",
      "{\"timestamp\": \"2026-01-17T05:46:32.164691Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from individual env var\"}\n",
      "{\"timestamp\": \"2026-01-17T05:46:32.164691Z\", \"level\": \"info\", \"event\": \"Loaded GOOGLE_API_KEY from individual env var\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_Kv...\", \"GOOGLE_API_KEY\": \"AIzaSy...\"}, \"timestamp\": \"2026-01-17T05:46:32.164691Z\", \"level\": \"info\", \"event\": \"API keys loaded\"}\n",
      "{\"config_keys\": [\"embedding_model\", \"retriever\", \"llm\"], \"timestamp\": \"2026-01-17T05:46:32.170480Z\", \"level\": \"info\", \"event\": \"YAML config loaded\"}\n",
      "{\"model\": \"models/text-embedding-004\", \"timestamp\": \"2026-01-17T05:46:32.170480Z\", \"level\": \"info\", \"event\": \"Loading embedding model\"}\n",
      "{\"session_id\": \"session_20260117_104627_3ea02899\", \"timestamp\": \"2026-01-17T05:46:32.207316Z\", \"level\": \"info\", \"event\": \"LCEL graph built successfully\"}\n",
      "{\"index_path\": \"faiss_index/session_20260117_104627_3ea02899\", \"index_name\": \"index\", \"search_type\": \"mmr\", \"k\": 5, \"fetch_k\": 20, \"lambda_mult\": 0.5, \"session_id\": \"session_20260117_104627_3ea02899\", \"timestamp\": \"2026-01-17T05:46:32.207316Z\", \"level\": \"info\", \"event\": \"FAISS retriever loaded successfully\"}\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0, model: gemini-2.0-flash\n",
      "Please retry in 27.584999652s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "}\n",
      "violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "}\n",
      "violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_input_token_count\"\n",
      "  quota_id: \"GenerateContentInputTokensPerModelPerMinute-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 27\n",
      "}\n",
      "].\n",
      "{\"error\": \"429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0, model: gemini-2.0-flash\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash\\nPlease retry in 25.373675905s. [links {\\n  description: \\\"Learn more about Gemini API quotas\\\"\\n  url: \\\"https://ai.google.dev/gemini-api/docs/rate-limits\\\"\\n}\\n, violations {\\n  quota_metric: \\\"generativelanguage.googleapis.com/generate_content_free_tier_input_token_count\\\"\\n  quota_id: \\\"GenerateContentInputTokensPerModelPerMinute-FreeTier\\\"\\n  quota_dimensions {\\n    key: \\\"model\\\"\\n    value: \\\"gemini-2.0-flash\\\"\\n  }\\n  quota_dimensions {\\n    key: \\\"location\\\"\\n    value: \\\"global\\\"\\n  }\\n}\\nviolations {\\n  quota_metric: \\\"generativelanguage.googleapis.com/generate_content_free_tier_requests\\\"\\n  quota_id: \\\"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\\\"\\n  quota_dimensions {\\n    key: \\\"model\\\"\\n    value: \\\"gemini-2.0-flash\\\"\\n  }\\n  quota_dimensions {\\n    key: \\\"location\\\"\\n    value: \\\"global\\\"\\n  }\\n}\\nviolations {\\n  quota_metric: \\\"generativelanguage.googleapis.com/generate_content_free_tier_requests\\\"\\n  quota_id: \\\"GenerateRequestsPerDayPerProjectPerModel-FreeTier\\\"\\n  quota_dimensions {\\n    key: \\\"model\\\"\\n    value: \\\"gemini-2.0-flash\\\"\\n  }\\n  quota_dimensions {\\n    key: \\\"location\\\"\\n    value: \\\"global\\\"\\n  }\\n}\\n, retry_delay {\\n  seconds: 25\\n}\\n]\", \"timestamp\": \"2026-01-17T05:46:35.555490Z\", \"level\": \"error\", \"event\": \"Failed to invoke ConversationalRAG\"}\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 401 Unauthorized\"\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run 8955c4ad-5cc2-42d9-988e-b95ea285fe0b: AuthenticationError(\"Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-********************************************************************************************************************************************************ob0A. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'code': 'invalid_api_key', 'param': None}, 'status': 401}\")\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1619, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 693, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 693, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 266, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 223, in evaluate_strings\n",
      "    return self._evaluate_strings(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\langchain\\evaluation\\qa\\eval_chain.py\", line 313, in _evaluate_strings\n",
      "    result = self(\n",
      "             ^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 189, in warning_emitting_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 410, in __call__\n",
      "    return self.invoke(\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 165, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
      "    response = self.generate([inputs], run_manager=run_manager)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
      "    return self.llm.generate_prompt(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 980, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 799, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 1045, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 689, in _generate\n",
      "    response = self.client.create(**payload)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 286, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 1147, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1259, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1047, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.AuthenticationError: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-********************************************************************************************************************************************************ob0A. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'code': 'invalid_api_key', 'param': None}, 'status': 401}\n",
      "2it [00:17,  8.58s/it]{\"timestamp\": \"2026-01-17T05:46:36.065292Z\", \"level\": \"info\", \"event\": \"Running in LOCAL mode: .env loaded\"}\n",
      "{\"timestamp\": \"2026-01-17T05:46:36.067730Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from individual env var\"}\n",
      "{\"timestamp\": \"2026-01-17T05:46:36.067730Z\", \"level\": \"info\", \"event\": \"Loaded GOOGLE_API_KEY from individual env var\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_Kv...\", \"GOOGLE_API_KEY\": \"AIzaSy...\"}, \"timestamp\": \"2026-01-17T05:46:36.070091Z\", \"level\": \"info\", \"event\": \"API keys loaded\"}\n",
      "{\"config_keys\": [\"embedding_model\", \"retriever\", \"llm\"], \"timestamp\": \"2026-01-17T05:46:36.073337Z\", \"level\": \"info\", \"event\": \"YAML config loaded\"}\n",
      "{\"session_id\": \"session_20260117_104636_51035a37\", \"temp_dir\": \"data\\\\session_20260117_104636_51035a37\", \"faiss_dir\": \"faiss_index\\\\session_20260117_104636_51035a37\", \"sessionized\": true, \"timestamp\": \"2026-01-17T05:46:36.073337Z\", \"level\": \"info\", \"event\": \"ChatIngestor initialized\"}\n",
      "{\"uploaded\": \"Physics 9th Ch 1 Final.pdf\", \"saved_as\": \"data\\\\session_20260117_104636_51035a37\\\\5ab4d277.pdf\", \"timestamp\": \"2026-01-17T05:46:36.078086Z\", \"level\": \"info\", \"event\": \"File saved for ingestion\"}\n",
      "{\"count\": 23, \"timestamp\": \"2026-01-17T05:46:37.134128Z\", \"level\": \"info\", \"event\": \"Documents loaded\"}\n",
      "{\"chunks\": 62, \"chunk_size\": 1000, \"overlap\": 200, \"timestamp\": \"2026-01-17T05:46:37.134591Z\", \"level\": \"info\", \"event\": \"Documents split\"}\n",
      "{\"model\": \"models/text-embedding-004\", \"timestamp\": \"2026-01-17T05:46:37.139951Z\", \"level\": \"info\", \"event\": \"Loading embedding model\"}\n",
      "{\"added\": 1, \"index\": \"faiss_index\\\\session_20260117_104636_51035a37\", \"timestamp\": \"2026-01-17T05:46:40.248480Z\", \"level\": \"info\", \"event\": \"FAISS index updated\"}\n",
      "{\"k\": 5, \"fetch_k\": 20, \"lambda_mult\": 0.5, \"timestamp\": \"2026-01-17T05:46:40.250679Z\", \"level\": \"info\", \"event\": \"Using MMR search\"}\n",
      "{\"timestamp\": \"2026-01-17T05:46:40.253072Z\", \"level\": \"info\", \"event\": \"Running in LOCAL mode: .env loaded\"}\n",
      "{\"timestamp\": \"2026-01-17T05:46:40.253072Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from individual env var\"}\n",
      "{\"timestamp\": \"2026-01-17T05:46:40.255937Z\", \"level\": \"info\", \"event\": \"Loaded GOOGLE_API_KEY from individual env var\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_Kv...\", \"GOOGLE_API_KEY\": \"AIzaSy...\"}, \"timestamp\": \"2026-01-17T05:46:40.255937Z\", \"level\": \"info\", \"event\": \"API keys loaded\"}\n",
      "{\"config_keys\": [\"embedding_model\", \"retriever\", \"llm\"], \"timestamp\": \"2026-01-17T05:46:40.255937Z\", \"level\": \"info\", \"event\": \"YAML config loaded\"}\n",
      "{\"provider\": \"google\", \"model\": \"gemini-2.0-flash\", \"timestamp\": \"2026-01-17T05:46:40.255937Z\", \"level\": \"info\", \"event\": \"Loading LLM\"}\n",
      "{\"session_id\": \"session_20260117_104636_51035a37\", \"timestamp\": \"2026-01-17T05:46:40.264487Z\", \"level\": \"info\", \"event\": \"LLM loaded successfully\"}\n",
      "{\"session_id\": \"session_20260117_104636_51035a37\", \"timestamp\": \"2026-01-17T05:46:40.266598Z\", \"level\": \"info\", \"event\": \"ConversationalRAG initialized\"}\n",
      "{\"timestamp\": \"2026-01-17T05:46:40.267770Z\", \"level\": \"info\", \"event\": \"Running in LOCAL mode: .env loaded\"}\n",
      "{\"timestamp\": \"2026-01-17T05:46:40.267770Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from individual env var\"}\n",
      "{\"timestamp\": \"2026-01-17T05:46:40.267770Z\", \"level\": \"info\", \"event\": \"Loaded GOOGLE_API_KEY from individual env var\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_Kv...\", \"GOOGLE_API_KEY\": \"AIzaSy...\"}, \"timestamp\": \"2026-01-17T05:46:40.267770Z\", \"level\": \"info\", \"event\": \"API keys loaded\"}\n",
      "{\"config_keys\": [\"embedding_model\", \"retriever\", \"llm\"], \"timestamp\": \"2026-01-17T05:46:40.272660Z\", \"level\": \"info\", \"event\": \"YAML config loaded\"}\n",
      "{\"model\": \"models/text-embedding-004\", \"timestamp\": \"2026-01-17T05:46:40.276091Z\", \"level\": \"info\", \"event\": \"Loading embedding model\"}\n",
      "{\"session_id\": \"session_20260117_104636_51035a37\", \"timestamp\": \"2026-01-17T05:46:40.303464Z\", \"level\": \"info\", \"event\": \"LCEL graph built successfully\"}\n",
      "{\"index_path\": \"faiss_index/session_20260117_104636_51035a37\", \"index_name\": \"index\", \"search_type\": \"mmr\", \"k\": 5, \"fetch_k\": 20, \"lambda_mult\": 0.5, \"session_id\": \"session_20260117_104636_51035a37\", \"timestamp\": \"2026-01-17T05:46:40.303464Z\", \"level\": \"info\", \"event\": \"FAISS retriever loaded successfully\"}\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0, model: gemini-2.0-flash\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash\n",
      "Please retry in 19.490552143s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_input_token_count\"\n",
      "  quota_id: \"GenerateContentInputTokensPerModelPerMinute-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "}\n",
      "violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "}\n",
      "violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 19\n",
      "}\n",
      "].\n",
      "{\"error\": \"429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0, model: gemini-2.0-flash\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash\\nPlease retry in 17.295385851s. [links {\\n  description: \\\"Learn more about Gemini API quotas\\\"\\n  url: \\\"https://ai.google.dev/gemini-api/docs/rate-limits\\\"\\n}\\n, violations {\\n  quota_metric: \\\"generativelanguage.googleapis.com/generate_content_free_tier_input_token_count\\\"\\n  quota_id: \\\"GenerateContentInputTokensPerModelPerMinute-FreeTier\\\"\\n  quota_dimensions {\\n    key: \\\"model\\\"\\n    value: \\\"gemini-2.0-flash\\\"\\n  }\\n  quota_dimensions {\\n    key: \\\"location\\\"\\n    value: \\\"global\\\"\\n  }\\n}\\nviolations {\\n  quota_metric: \\\"generativelanguage.googleapis.com/generate_content_free_tier_requests\\\"\\n  quota_id: \\\"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\\\"\\n  quota_dimensions {\\n    key: \\\"model\\\"\\n    value: \\\"gemini-2.0-flash\\\"\\n  }\\n  quota_dimensions {\\n    key: \\\"location\\\"\\n    value: \\\"global\\\"\\n  }\\n}\\nviolations {\\n  quota_metric: \\\"generativelanguage.googleapis.com/generate_content_free_tier_requests\\\"\\n  quota_id: \\\"GenerateRequestsPerDayPerProjectPerModel-FreeTier\\\"\\n  quota_dimensions {\\n    key: \\\"model\\\"\\n    value: \\\"gemini-2.0-flash\\\"\\n  }\\n  quota_dimensions {\\n    key: \\\"location\\\"\\n    value: \\\"global\\\"\\n  }\\n}\\n, retry_delay {\\n  seconds: 17\\n}\\n]\", \"timestamp\": \"2026-01-17T05:46:43.649730Z\", \"level\": \"error\", \"event\": \"Failed to invoke ConversationalRAG\"}\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 401 Unauthorized\"\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run 556d3a3f-22ae-49d8-8bf5-ee8dcb66c114: AuthenticationError(\"Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-********************************************************************************************************************************************************ob0A. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'code': 'invalid_api_key', 'param': None}, 'status': 401}\")\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1619, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 693, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 693, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 266, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 223, in evaluate_strings\n",
      "    return self._evaluate_strings(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\langchain\\evaluation\\qa\\eval_chain.py\", line 313, in _evaluate_strings\n",
      "    result = self(\n",
      "             ^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 189, in warning_emitting_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 410, in __call__\n",
      "    return self.invoke(\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 165, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
      "    response = self.generate([inputs], run_manager=run_manager)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
      "    return self.llm.generate_prompt(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 980, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 799, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 1045, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 689, in _generate\n",
      "    response = self.client.create(**payload)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 286, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 1147, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1259, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Machine learning\\llmops_series\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1047, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.AuthenticationError: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-********************************************************************************************************************************************************ob0A. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'code': 'invalid_api_key', 'param': None}, 'status': 401}\n",
      "3it [00:25,  8.64s/it]\n"
     ]
    }
   ],
   "source": [
    "from langsmith.evaluation import evaluate, LangChainStringEvaluator\n",
    "\n",
    "qa_evaluator = [LangChainStringEvaluator(\"cot_qa\")]\n",
    "dataset_name = \"llmops_dataset_new\"\n",
    "\n",
    "# Run evaluation using our RAG function\n",
    "experiment_results = evaluate(\n",
    "    answer_ai_report_question,\n",
    "    data=dataset_name,\n",
    "    evaluators=qa_evaluator,\n",
    "    experiment_prefix=\"test-llmops_dataset-qa-rag\",\n",
    "    metadata={\n",
    "        \"variant\": \"RAG with FAISS and AI Engineering Report\",\n",
    "        \"chunk_size\": 1000,\n",
    "        \"chunk_overlap\": 200,\n",
    "        \"k\": 5\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c9b102",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmops-series (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
